{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast data 256x256 CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noqa\n",
    "import os\n",
    "COLAB = 'DATALAB_DEBUG' in os.environ\n",
    "\n",
    "if COLAB:\n",
    "    #!apt-get update && apt-get install git\n",
    "    !rm -rf bstrap\n",
    "    !git clone https://gist.github.com/oskopek/e27ca34cb2b813cae614520e8374e741 bstrap\n",
    "    \n",
    "    import bstrap.bootstrap as bootstrap\n",
    "    import bstrap.drive_utils as drive_utils\n",
    "    drive_u = drive_utils\n",
    "\n",
    "else:\n",
    "    wd = %%pwd\n",
    "    if wd.endswith('notebooks'):\n",
    "        print('Current directory:', wd)\n",
    "        %cd ..\n",
    "        %pwd\n",
    "    \n",
    "    import resources.our_colab_utils.bootstrap as bootstrap\n",
    "    drive_u = None\n",
    "\n",
    "bootstrap.bootstrap(branch='master', packages='dotmap==1.2.20 keras==2.1.4 pydicom==1.0.2 Pillow==5.0.0', drive_utils=drive_u)\n",
    "\n",
    "if COLAB:\n",
    "    !rm -rf bstrap\n",
    "else:\n",
    "   !pip install dotmap --user # Azure hack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noqa\n",
    "\n",
    "# Train data\n",
    "if COLAB:\n",
    "    # Download data\n",
    "    fname = \"small_all_256x256_augmented.zip\"\n",
    "    DATA_IN_FOLDER = os.path.join(\".\", \"data_in\")\n",
    "    if not os.path.exists(DATA_IN_FOLDER):\n",
    "        os.mkdir(DATA_IN_FOLDER)\n",
    "    zip_path = drive_utils.find_download(drive_utils.get_service(), fname)\n",
    "    os.rename(zip_path, os.path.join(DATA_IN_FOLDER, fname))\n",
    "    !unzip \"data_in/small_all_256x256_augmented.zip\" >/dev/null\n",
    "    !rm -rf data_in/small_all_256x256\n",
    "    !mv small_all_256x256 data_in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noqa\n",
    "\n",
    "# Test data\n",
    "if COLAB:\n",
    "    # Download data\n",
    "    fname = \"small_all_256x256.zip\"\n",
    "    DATA_IN_FOLDER = os.path.join(\".\", \"data_in\")\n",
    "    if not os.path.exists(DATA_IN_FOLDER):\n",
    "        os.mkdir(DATA_IN_FOLDER)\n",
    "    zip_path = drive_utils.find_download(drive_utils.get_service(), fname)\n",
    "    os.rename(zip_path, os.path.join(DATA_IN_FOLDER, fname))\n",
    "    !unzip \"data_in/small_all_256x256.zip\" >/dev/null\n",
    "    !rm -rf data_in/small_all_256x256_test\n",
    "    !mv small_all_256x256 data_in/small_all_256x256_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from models.base import BaseModel\n",
    "from resources.data.utils import next_batch, shuffle\n",
    "from resources.model_utils import tile_images\n",
    "\n",
    "# Flags\n",
    "from flags import flags_parser\n",
    "flags_parser.parse('flags/cyclegan.json', None)\n",
    "FLAGS = flags_parser.FLAGS\n",
    "assert FLAGS is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "ngf = 32\n",
    "ndf = 64\n",
    "\n",
    "\n",
    "# Copied from Cycle-GAN implmentation\n",
    "def instance_norm(x):\n",
    "    with tf.variable_scope(\"instance_norm\"):\n",
    "        epsilon = 1e-5\n",
    "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
    "        scale = tf.get_variable(\n",
    "            'scale', [x.get_shape()[-1]], initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n",
    "        offset = tf.get_variable('offset', [x.get_shape()[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        out = scale * tf.div(x - mean, tf.sqrt(var + epsilon)) + offset\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def conv2d(x, filters=3, kernel=3, strides=1, padding='VALID', relu=0.2, norm=True, name='conv'):\n",
    "    with tf.variable_scope(name):\n",
    "        out_res = tf.layers.conv2d(\n",
    "            x,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        if norm:\n",
    "            out_res = instance_norm(out_res)\n",
    "        if relu > 0:\n",
    "            out_res = tf.nn.leaky_relu(out_res, alpha=relu)\n",
    "        elif relu == 0:\n",
    "            out_res = tf.nn.relu(out_res)\n",
    "        return out_res\n",
    "\n",
    "\n",
    "def deconv2d(x, outshape, filters=64, kernel=7, strides=1, padding=\"VALID\", name=\"deconv2d\", norm=True, relu=0.2):\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        conv = tf.layers.conv2d_transpose(\n",
    "            x,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "        if norm:\n",
    "            conv = instance_norm(conv)\n",
    "\n",
    "        if relu == 0:\n",
    "            conv = tf.nn.relu(conv, \"relu\")\n",
    "        elif relu > 0:\n",
    "            conv = tf.nn.leaky_relu(conv, alpha=relu)\n",
    "\n",
    "        return conv\n",
    "\n",
    "\n",
    "def build_resnet_block(x, dim, name=\"resnet\", padding='REFLECT'):\n",
    "    with tf.variable_scope(name):\n",
    "        with tf.variable_scope(\"Conv1\"):\n",
    "            out_res = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], padding)\n",
    "            out_res = conv2d(out_res, filters=dim, name=\"conv1\")\n",
    "        with tf.variable_scope(\"Conv2\"):\n",
    "            out_res = tf.pad(out_res, [[0, 0], [1, 1], [1, 1], [0, 0]], padding)\n",
    "            out_res = conv2d(out_res, filters=dim, name=\"conv2\", relu=-1)\n",
    "\n",
    "        return tf.nn.relu(out_res + x)\n",
    "\n",
    "\n",
    "def build_generator_resnet_9blocks(x, name=\"generator\", skip=True):\n",
    "    with tf.variable_scope(name):\n",
    "        f = 7\n",
    "        ks = 3\n",
    "        padding = \"CONSTANT\"\n",
    "\n",
    "        pad_input = tf.pad(x, [[0, 0], [ks, ks], [ks, ks], [0, 0]], padding)\n",
    "        o_c1 = conv2d(pad_input, filters=ngf, kernel=f, name=\"conv1\")\n",
    "        o_c2 = conv2d(o_c1, filters=ngf * 2, kernel=ks, strides=2, padding=\"SAME\", name=\"conv2\")\n",
    "        o_c3 = conv2d(o_c2, filters=ngf * 4, kernel=ks, strides=2, padding=\"SAME\", name=\"conv3\")\n",
    "\n",
    "        o_r = build_resnet_block(o_c3, ngf * 4, \"r1\", padding)\n",
    "        for i in range(2, 10):\n",
    "            o_r = build_resnet_block(o_r, ngf * 4, name=\"r\" + str(i), padding=padding)\n",
    "\n",
    "        o_c4 = deconv2d(\n",
    "            o_r, [BATCH_SIZE, 128, 128, ngf * 2], filters=ngf * 2, kernel=ks, strides=2, padding=\"SAME\", name=\"dc4\")\n",
    "        o_c5 = deconv2d(\n",
    "            o_c4, [BATCH_SIZE, 256, 256, ngf], filters=ngf, kernel=ks, strides=2, padding=\"SAME\", name=\"dc5\")\n",
    "        o_c6 = conv2d(o_c5, filters=1, kernel=f, padding=\"SAME\", name=\"c6\", norm=False, relu=-1)\n",
    "\n",
    "        if skip is True:\n",
    "            out_gen = tf.nn.tanh(x + o_c6, \"t1\")\n",
    "        else:\n",
    "            out_gen = tf.nn.tanh(o_c6, \"t1\")\n",
    "\n",
    "        return out_gen\n",
    "\n",
    "\n",
    "def discriminator(x, name=\"discriminator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f = 4\n",
    "\n",
    "        padw = 2\n",
    "\n",
    "        with tf.variable_scope('Conv1'):\n",
    "            pad_input = tf.pad(x, [[0, 0], [padw, padw], [padw, padw], [0, 0]], \"CONSTANT\")\n",
    "            o_c1 = conv2d(pad_input, filters=ndf, kernel=f, strides=2, name=\"c1\", norm=False)\n",
    "\n",
    "        with tf.variable_scope('Conv2'):\n",
    "            pad_o_c1 = tf.pad(o_c1, [[0, 0], [padw, padw], [padw, padw], [0, 0]], \"CONSTANT\")\n",
    "            o_c2 = conv2d(pad_o_c1, filters=ndf * 2, kernel=f, strides=2, name=\"c2\")\n",
    "\n",
    "        with tf.variable_scope('Conv3'):\n",
    "            pad_o_c2 = tf.pad(o_c2, [[0, 0], [padw, padw], [padw, padw], [0, 0]], \"CONSTANT\")\n",
    "            o_c3 = conv2d(pad_o_c2, filters=ndf * 4, kernel=f, strides=2, name=\"c3\")\n",
    "\n",
    "        with tf.variable_scope('Conv4'):\n",
    "            pad_o_c3 = tf.pad(o_c3, [[0, 0], [padw, padw], [padw, padw], [0, 0]], \"CONSTANT\")\n",
    "            o_c4 = conv2d(pad_o_c3, filters=ndf * 8, kernel=f, strides=1, name=\"c4\")\n",
    "\n",
    "        with tf.variable_scope('Conv5'):\n",
    "            pad_o_c4 = tf.pad(o_c4, [[0, 0], [padw, padw], [padw, padw], [0, 0]], \"CONSTANT\")\n",
    "            o_c5 = conv2d(pad_o_c4, filters=1, kernel=f, name=\"c5\", norm=False, relu=-1)\n",
    "\n",
    "        return o_c5\n",
    "\n",
    "\n",
    "def patch_discriminator(x, name=\"discriminator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f = 4\n",
    "\n",
    "        patch_input = tf.random_crop(x, [1, 70, 70, 3])\n",
    "        o_c1 = conv2d(patch_input, filters=ndf, kernel=f, strides=2, padding=\"SAME\", name=\"c1\", norm=False)\n",
    "        o_c2 = conv2d(o_c1, filters=ndf * 2, kernel=f, strides=2, padding=\"SAME\", name=\"c2\")\n",
    "        o_c3 = conv2d(o_c2, filters=ndf * 4, kernel=f, strides=2, padding=\"SAME\", name=\"c3\")\n",
    "        o_c4 = conv2d(o_c3, filters=ndf * 8, kernel=f, strides=2, padding=\"SAME\", name=\"c4\")\n",
    "        o_c5 = conv2d(o_c4, filters=1, kernel=f, padding=\"SAME\", name=\"c5\", norm=False, relu=-1)\n",
    "\n",
    "        return o_c5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputs, skip=True):\n",
    "    images_a = inputs['images_a']\n",
    "    images_b = inputs['images_b']\n",
    "\n",
    "    fake_pool_a = inputs['fake_pool_a']\n",
    "    fake_pool_b = inputs['fake_pool_b']\n",
    "\n",
    "    with tf.variable_scope(\"Model\") as scope:\n",
    "        prob_real_a_is_real = discriminator(images_a, \"d_A\")\n",
    "        prob_real_b_is_real = discriminator(images_b, \"d_B\")\n",
    "        generator = build_generator_resnet_9blocks\n",
    "\n",
    "        fake_images_b = generator(images_a, name=\"g_A\", skip=skip)\n",
    "        fake_images_a = generator(images_b, name=\"g_B\", skip=skip)\n",
    "\n",
    "        scope.reuse_variables()\n",
    "\n",
    "        prob_fake_a_is_real = discriminator(fake_images_a, \"d_A\")\n",
    "        prob_fake_b_is_real = discriminator(fake_images_b, \"d_B\")\n",
    "\n",
    "        cycle_images_a = generator(fake_images_b, \"g_B\", skip=skip)\n",
    "        cycle_images_b = generator(fake_images_a, \"g_A\", skip=skip)\n",
    "\n",
    "        scope.reuse_variables()\n",
    "\n",
    "        prob_fake_pool_a_is_real = discriminator(fake_pool_a, \"d_A\")\n",
    "        prob_fake_pool_b_is_real = discriminator(fake_pool_b, \"d_B\")\n",
    "\n",
    "    return {\n",
    "        'prob_real_a_is_real': prob_real_a_is_real,\n",
    "        'prob_real_b_is_real': prob_real_b_is_real,\n",
    "        'prob_fake_a_is_real': prob_fake_a_is_real,\n",
    "        'prob_fake_b_is_real': prob_fake_b_is_real,\n",
    "        'prob_fake_pool_a_is_real': prob_fake_pool_a_is_real,\n",
    "        'prob_fake_pool_b_is_real': prob_fake_pool_b_is_real,\n",
    "        'cycle_images_a': cycle_images_a,\n",
    "        'cycle_images_b': cycle_images_b,\n",
    "        'fake_images_a': fake_images_a,\n",
    "        'fake_images_b': fake_images_b,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut for loss calculation.\n",
    "def cycle_consistency_loss(real_images, generated_images):\n",
    "    \"\"\"L1-norm difference between the real and generated images.\"\"\"\n",
    "    return tf.reduce_mean(tf.abs(real_images - generated_images))\n",
    "\n",
    "\n",
    "def lsgan_loss_generator(prob_fake_is_real):\n",
    "    \"\"\"Least-squares generator loss\"\"\"\n",
    "    return tf.reduce_mean(tf.squared_difference(prob_fake_is_real, 1))\n",
    "\n",
    "\n",
    "def lsgan_loss_discriminator(prob_real_is_real, prob_fake_is_real):\n",
    "    \"\"\"Least-squares discriminator losses.\"\"\"\n",
    "    return (tf.reduce_mean(tf.squared_difference(prob_real_is_real, 1)) + tf.reduce_mean(\n",
    "        tf.squared_difference(prob_fake_is_real, 0))) * 0.5\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits=None, labels=None):\n",
    "    #return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return tf.reduce_mean(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_small(folder):\n",
    "\n",
    "    def read_all(folder):\n",
    "        images = []\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".dat\"):\n",
    "                fpath = os.path.join(folder, file)\n",
    "                images.append(np.fromfile(fpath))\n",
    "        images = np.asarray(images)\n",
    "        return images\n",
    "\n",
    "    healthy_f = os.path.join(folder, \"healthy\")\n",
    "    cancer_f = os.path.join(folder, \"cancer\")\n",
    "    healthy = read_all(healthy_f)\n",
    "    cancer = read_all(cancer_f)\n",
    "    healthy = np.reshape(healthy, (-1, 256, 256))\n",
    "    cancer = np.reshape(cancer, (-1, 256, 256))\n",
    "    healthy = np.expand_dims(healthy, -1)\n",
    "    cancer = np.expand_dims(cancer, -1)\n",
    "    return healthy, cancer\n",
    "\n",
    "\n",
    "# healthy, cancer = read_small(\"data_in/small_all_256x256\")\n",
    "# print(healthy.shape, cancer.shape)\n",
    "# healthy_test, cancer_test = read_small(\"data_in/small_all_256x256_test\")\n",
    "# print(healthy_test.shape, cancer_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class CycleGan(BaseModel):\n",
    "    # Setup constants\n",
    "    IMAGE_SIZE = 28\n",
    "    NEW_IMAGE_SIZE = 256\n",
    "    IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "    NOISE_SIZE = 100\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CycleGan, self).__init__(\n",
    "            logdir_name=FLAGS.data.out_dir,\n",
    "            checkpoint_dirname=FLAGS.training.checkpoint_dir,\n",
    "            expname=\"Cycle-GAN\",\n",
    "            threads=FLAGS.training.threads,\n",
    "            seed=FLAGS.training.seed)\n",
    "        with self.session.graph.as_default():\n",
    "            self._build()\n",
    "            self._init_variables()\n",
    "\n",
    "            self.fake_images_A = np.zeros((FLAGS.model.optimization.pool_size, 1, self.NEW_IMAGE_SIZE,\n",
    "                                           self.NEW_IMAGE_SIZE, 1))\n",
    "            self.fake_images_B = np.zeros((FLAGS.model.optimization.pool_size, 1, self.NEW_IMAGE_SIZE,\n",
    "                                           self.NEW_IMAGE_SIZE, 1))\n",
    "            self.summary_writer = tf.summary.FileWriter(self.logdir, flush_secs=5 * 1000)\n",
    "\n",
    "    # Construct the graph\n",
    "    def _build(self):\n",
    "        self.d_step = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"d_step\")\n",
    "        self.g_step = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"g_step\")\n",
    "\n",
    "        self.input_a = tf.placeholder(tf.float32, shape=(None, self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE, 1))\n",
    "        print(self.input_a.get_shape())\n",
    "        self.images_input_a = self.input_a  #tf.reshape(self.input_a, (-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1))\n",
    "        #print(self.images_input_a.get_shape())\n",
    "        #self.images_input_a = tf.image.resize_images(self.images_input_a, [self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE])\n",
    "        #print(self.images_input_a.get_shape())\n",
    "        #self.images_input_a = (self.images_input_a - 0.5) * 2.0\n",
    "        print(self.images_input_a.get_shape())\n",
    "\n",
    "        self.input_b = tf.placeholder(tf.float32, shape=(None, self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE, 1))\n",
    "        print(self.input_b.get_shape())\n",
    "        self.images_input_b = self.input_b  #tf.reshape(self.input_b, (-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1))\n",
    "        #print(self.images_input_b.get_shape())\n",
    "        #self.images_input_b = tf.image.resize_images(self.images_input_b, [self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE])\n",
    "        #print(self.images_input_b.get_shape())\n",
    "        #self.images_input_b = (self.images_input_b - 0.5) * 2.0\n",
    "        print(self.images_input_b.get_shape())\n",
    "\n",
    "        self.fake_pool_A = tf.placeholder(\n",
    "            tf.float32, [None, self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE, 1], name=\"fake_pool_A\")\n",
    "        self.fake_pool_B = tf.placeholder(\n",
    "            tf.float32, [None, self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE, 1], name=\"fake_pool_B\")\n",
    "\n",
    "        inputs = {\n",
    "            'images_a': self.images_input_a,\n",
    "            'images_b': self.images_input_b,\n",
    "            'fake_pool_a': self.fake_pool_A,\n",
    "            'fake_pool_b': self.fake_pool_B\n",
    "        }\n",
    "\n",
    "        self.training = tf.placeholder_with_default(False, shape=())\n",
    "        self.noise_input_interpolated = tf.placeholder(tf.float32, shape=(None, self.NOISE_SIZE))\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
    "        self.num_fake_inputs = 0\n",
    "\n",
    "        self.outputs = outputs = build_model(inputs, skip=True)\n",
    "\n",
    "        self.prob_real_a_is_real = outputs['prob_real_a_is_real']\n",
    "        self.prob_real_b_is_real = outputs['prob_real_b_is_real']\n",
    "        self.fake_images_a = outputs['fake_images_a']\n",
    "        self.fake_images_b = outputs['fake_images_b']\n",
    "        self.prob_fake_a_is_real = outputs['prob_fake_a_is_real']\n",
    "        self.prob_fake_b_is_real = outputs['prob_fake_b_is_real']\n",
    "\n",
    "        self.cycle_images_a = outputs['cycle_images_a']\n",
    "        self.cycle_images_b = outputs['cycle_images_b']\n",
    "\n",
    "        self.prob_fake_pool_a_is_real = outputs['prob_fake_pool_a_is_real']\n",
    "        self.prob_fake_pool_b_is_real = outputs['prob_fake_pool_b_is_real']\n",
    "\n",
    "        # Losses\n",
    "        print(self.images_input_a.get_shape())\n",
    "        print(self.cycle_images_a.get_shape())\n",
    "        print(FLAGS.model.optimization.lambda_a)\n",
    "        cycle_consistency_loss_a = FLAGS.model.optimization.lambda_a * cycle_consistency_loss(\n",
    "            real_images=self.images_input_a, generated_images=self.cycle_images_a)\n",
    "        cycle_consistency_loss_b = FLAGS.model.optimization.lambda_b * cycle_consistency_loss(\n",
    "            real_images=self.images_input_b, generated_images=self.cycle_images_b)\n",
    "\n",
    "        lsgan_loss_a = lsgan_loss_generator(self.prob_fake_a_is_real)\n",
    "        lsgan_loss_b = lsgan_loss_generator(self.prob_fake_b_is_real)\n",
    "\n",
    "        g_loss_A = cycle_consistency_loss_a + cycle_consistency_loss_b + lsgan_loss_b\n",
    "        g_loss_B = cycle_consistency_loss_a + cycle_consistency_loss_b + lsgan_loss_a\n",
    "\n",
    "        d_loss_A = lsgan_loss_discriminator(\n",
    "            prob_real_is_real=self.prob_real_a_is_real, prob_fake_is_real=self.prob_fake_pool_a_is_real)\n",
    "        d_loss_B = lsgan_loss_discriminator(\n",
    "            prob_real_is_real=self.prob_real_b_is_real, prob_fake_is_real=self.prob_fake_pool_b_is_real)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate, beta1=0.5)\n",
    "\n",
    "        self.model_vars = tf.trainable_variables()\n",
    "\n",
    "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
    "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
    "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
    "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
    "\n",
    "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
    "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
    "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
    "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
    "\n",
    "        # Summary variables for tensorboard\n",
    "        self.g_A_loss_summ = tf.summary.scalar(\"loss/generator_A\", g_loss_A)\n",
    "        self.g_B_loss_summ = tf.summary.scalar(\"loss/generator_B\", g_loss_B)\n",
    "        self.d_A_loss_summ = tf.summary.scalar(\"loss/discriminator_A\", d_loss_A)\n",
    "        self.d_B_loss_summ = tf.summary.scalar(\"loss/discriminator_B\", d_loss_B)\n",
    "\n",
    "        # Test summaries\n",
    "        results = tf.stack([\n",
    "            self.images_input_a, self.fake_images_b, self.cycle_images_a, self.images_input_b, self.fake_images_a,\n",
    "            self.cycle_images_b\n",
    "        ],\n",
    "                           axis=0)\n",
    "        tiled_image_random = tile_images(results, 3, 2, self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE)\n",
    "        image_summary_op = tf.summary.image('generated_images', tiled_image_random, max_outputs=1)\n",
    "\n",
    "        results_diff = tf.stack([\n",
    "            self.fake_images_b - self.images_input_a, self.cycle_images_a - self.fake_images_b,\n",
    "            self.fake_images_a - self.images_input_b, self.cycle_images_b - self.fake_images_a\n",
    "        ],\n",
    "                                axis=0)\n",
    "        tiled_image_random_diff = tile_images(results_diff, 2, 2, self.NEW_IMAGE_SIZE, self.NEW_IMAGE_SIZE)\n",
    "        image_summary_op_diff = tf.summary.image('generated_images_diff', tiled_image_random_diff, max_outputs=1)\n",
    "        self.gen_image_summary_op = tf.summary.merge([image_summary_op, image_summary_op_diff])\n",
    "\n",
    "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
    "        \"\"\"\n",
    "        This function saves the generated image to corresponding\n",
    "        pool of images.\n",
    "        It keeps on feeling the pool till it is full and then randomly\n",
    "        selects an already stored image and replace it with new one.\n",
    "        \"\"\"\n",
    "        if num_fakes < FLAGS.model.optimization.pool_size:\n",
    "            fake_pool[num_fakes] = fake\n",
    "            return fake\n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                random_id = random.randint(0, FLAGS.model.optimization.pool_size - 1)\n",
    "                temp = fake_pool[random_id]\n",
    "                fake_pool[random_id] = fake\n",
    "                return temp\n",
    "            else:\n",
    "                return fake\n",
    "\n",
    "    def train_batch(self, batch, step, curr_lr):\n",
    "        BATCH_SIZE = FLAGS.model.optimization.batch_size\n",
    "\n",
    "        # Optimizing the G_A network\n",
    "        _, fake_B_temp, summary_str = self.session.run([self.g_A_trainer, self.fake_images_b, self.g_A_loss_summ],\n",
    "                                                       feed_dict={\n",
    "                                                           self.input_a: batch['images_a'],\n",
    "                                                           self.input_b: batch['images_b'],\n",
    "                                                           self.learning_rate: curr_lr\n",
    "                                                       })\n",
    "        self.summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "        fake_B_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
    "\n",
    "        # Optimizing the D_B network\n",
    "        _, summary_str = self.session.run(\n",
    "            [self.d_B_trainer, self.d_B_loss_summ],\n",
    "            feed_dict={\n",
    "                self.input_a: batch['images_a'],\n",
    "                self.input_b: batch['images_b'],\n",
    "                self.learning_rate: curr_lr,\n",
    "                self.fake_pool_B: fake_B_temp1\n",
    "            })\n",
    "        self.summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "        # Optimizing the G_B network\n",
    "        _, fake_A_temp, summary_str = self.session.run([self.g_B_trainer, self.fake_images_a, self.g_B_loss_summ],\n",
    "                                                       feed_dict={\n",
    "                                                           self.input_a: batch['images_a'],\n",
    "                                                           self.input_b: batch['images_b'],\n",
    "                                                           self.learning_rate: curr_lr\n",
    "                                                       })\n",
    "        self.summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "        fake_A_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
    "\n",
    "        # Optimizing the D_A network\n",
    "        _, summary_str = self.session.run(\n",
    "            [self.d_A_trainer, self.d_A_loss_summ],\n",
    "            feed_dict={\n",
    "                self.input_a: batch['images_a'],\n",
    "                self.input_b: batch['images_b'],\n",
    "                self.learning_rate: curr_lr,\n",
    "                self.fake_pool_A: fake_A_temp1\n",
    "            })\n",
    "        self.summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "        self.summary_writer.flush()\n",
    "        self.num_fake_inputs += 1\n",
    "\n",
    "    # Generate images from test noise\n",
    "    def test_eval(self, image_a, image_b, step):\n",
    "        summary = self.session.run(self.gen_image_summary_op, feed_dict={self.input_a: image_a, self.input_b: image_b})\n",
    "        self.summary_writer.add_summary(summary, step)\n",
    "\n",
    "    # Do not use! Fills up hard drive, and not needed.\n",
    "    def test_eval_img(self, n_epochs):\n",
    "        BATCH_SIZE = FLAGS.model.optimization.batch_size\n",
    "\n",
    "        healthy_test, cancer_test = read_small(\"data_in/small_all_256x256_test\")\n",
    "\n",
    "        data_out_dir = os.path.join(self.logdir, 'out_' + str(n_epochs))\n",
    "        if not os.path.exists(data_out_dir):\n",
    "            os.makedirs(data_out_dir)\n",
    "\n",
    "        cancer_genb = np.zeros((0, 256, 256, 1))\n",
    "        cancer_cycleb = np.zeros((0, 256, 256, 1))\n",
    "        healthy_genb = np.zeros((0, 256, 256, 1))\n",
    "        healthy_cycleb = np.zeros((0, 256, 256, 1))\n",
    "\n",
    "        epoch_steps = min(healthy_test.shape[0], cancer_test.shape[0])\n",
    "        for n_batch, (batch_a, batch_b) in enumerate(\n",
    "                zip(next_batch(healthy_test, BATCH_SIZE), next_batch(cancer_test, BATCH_SIZE))):\n",
    "            if batch_a.shape[0] == 0 or batch_b.shape[0] == 0:\n",
    "                break\n",
    "            cancer_gen, healthy_cycle, \\\n",
    "            healthy_gen, cancer_cycle = self.session.run([self.fake_images_b, self.cycle_images_a,\n",
    "                                                         self.fake_images_a, self.cycle_images_b],\n",
    "                                                         feed_dict={self.input_a: batch_a, self.input_b: batch_b})\n",
    "            cancer_genb = np.concatenate([cancer_genb, cancer_gen], axis=0)\n",
    "            cancer_cycleb = np.concatenate([cancer_cycleb, cancer_cycle], axis=0)\n",
    "            healthy_genb = np.concatenate([healthy_genb, healthy_gen], axis=0)\n",
    "            healthy_cycleb = np.concatenate([healthy_cycleb, healthy_cycle], axis=0)\n",
    "\n",
    "        cancer_genb.tofile(os.path.join(data_out_dir, 'cancer_gen.dat'))\n",
    "        healthy_cycleb.tofile(os.path.join(data_out_dir, 'healthy_cycle.dat'))\n",
    "        healthy_genb.tofile(os.path.join(data_out_dir, 'healthy_gen.dat'))\n",
    "        cancer_cycleb.tofile(os.path.join(data_out_dir, 'cancer_cycle.dat'))\n",
    "\n",
    "    def run(self):\n",
    "        BATCH_SIZE = 1\n",
    "        base_lr = 0.0002\n",
    "\n",
    "        # Iterate through epochs\n",
    "        for epoch in range(FLAGS.model.optimization.epochs):\n",
    "            print(\"Epoch %d\" % epoch, flush=True)\n",
    "            if epoch < 100:\n",
    "                curr_lr = base_lr\n",
    "            else:\n",
    "                curr_lr = base_lr - base_lr * (epoch - 100) / 100\n",
    "\n",
    "            # Read it every time to not skip some healthy images\n",
    "            # Due to shuffling method.\n",
    "            healthy, cancer = read_small(\"data_in/small_all_256x256\")\n",
    "            cancer, healthy = shuffle(cancer, healthy)\n",
    "            epoch_steps = min(healthy.shape[0], cancer.shape[0])\n",
    "            for n_batch, (batch_a, batch_b) in enumerate(\n",
    "                    zip(next_batch(healthy, BATCH_SIZE), next_batch(cancer, BATCH_SIZE))):\n",
    "                if batch_a.shape[0] == 0 or batch_b.shape[0] == 0:\n",
    "                    break\n",
    "                step = epoch * epoch_steps + n_batch\n",
    "\n",
    "                inputs = {\"images_a\": batch_a, \"images_b\": batch_b}\n",
    "                self.train_batch(inputs, step, curr_lr)\n",
    "\n",
    "                # Test noise\n",
    "                if n_batch % FLAGS.training.log_interval == 0:\n",
    "                    self.test_eval(batch_a, batch_b, step)\n",
    "                if n_batch % 1000 == 0:\n",
    "                    self.saver.save(self.session, os.path.join(self.logdir, FLAGS.training.checkpoint_dir,\n",
    "                                                               \"model.ckpt\"))\n",
    "\n",
    "            # End of epoch\n",
    "            if epoch % FLAGS.training.save_interval == 0:\n",
    "                self.saver.save(self.session, os.path.join(self.logdir, FLAGS.training.checkpoint_dir, \"model.ckpt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "CycleGan().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
